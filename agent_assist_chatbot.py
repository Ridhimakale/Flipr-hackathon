# -*- coding: utf-8 -*-
"""agent_assist_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yGiWne5XHnqRLqib142PLeJ12gI9bZn_
"""

!pip install -q langchain huggingface_hub transformers faiss-cpu sentence-transformers pypdf openai tiktoken

!pip install -q langchain huggingface_hub transformers faiss-cpu sentence-transformers pypdf openai tiktoken langchain-community

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFaceHub
from langchain.prompts import PromptTemplate
import os

# Set Hugging Face API Key (Replace with your token)
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

from huggingface_hub import InferenceClient

client = InferenceClient(
    provider="hf-inference",
    api_key="hf_xxxxxxxxxxxxxxxxxxxxxxxx",
)

completion = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What is the capital of France?"
        }
    ],
)

print(completion.choices[0].message)

# Load the Amazon Support KB PDF
loader = PyPDFLoader("amazon_support_kb.pdf")  # Upload this file to Colab
pages = loader.load()

# Split into chunks for better retrieval
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", "=====", "Q:", "A:"]
)
texts = text_splitter.split_documents(pages)

# Use Hugging Face embeddings (efficient for semantic search)
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Create a FAISS vector store
db = FAISS.from_documents(texts, embeddings)

# Option 1: Hugging Face Zephyr (Free)
llm = HuggingFaceHub(
    repo_id="HuggingFaceH4/zephyr-7b-beta",
    model_kwargs={"temperature": 0.3, "max_length": 512}
)

# Option 2: OpenAI (Paid, better accuracy)
# os.environ["OPENAI_API_KEY"] = "your_openai_key"
# from langchain.llms import OpenAI
# llm = OpenAI(model="gpt-3.5-turbo-instruct")

def get_kb_answer(query):
    retriever = db.as_retriever(search_kwargs={"k": 3})
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    result = qa_chain({"query": query})
    return {
        "answer": result["result"],
        "sources": [doc.metadata["page"] for doc in result["source_documents"]]
    }

def draft_agent_response(conversation):
    prompt = PromptTemplate(
        input_variables=["conversation"],
        template="""
        You are an Amazon support agent. Draft a concise, professional response based on this conversation:

        {conversation}

        Key points to include:
        - Empathize with the customer (e.g., "I understand this is frustrating")
        - Provide a clear solution from the KB
        - Use bullet points if multiple steps are needed
        - End with a call-to-action (e.g., "Let me know if you need further help!")

        Response:
        """
    )
    return llm(prompt.format(conversation=conversation))

def summarize_ticket(ticket_text):
    prompt = f"""
    Summarize this Amazon support ticket for quick agent review:

    {ticket_text}

    Include:
    - Customer's primary issue (1 sentence)
    - Urgency level (Low/Medium/High)
    - Suggested KB category (e.g., "Returns", "Payments")
    - Key details (order ID, contact method, etc.)

    Summary:
    """
    return llm(prompt)

# Example 1: Retrieve KB answer
query = "How do I reset my Amazon password?"
answer = get_kb_answer(query)
print("Answer:", answer["answer"])
print("Source Pages:", answer["sources"])

# Example 2: Draft a response
conversation = """
Customer: I never received my order #12345. It was supposed to arrive yesterday.
Agent: Let me check the tracking details.
Customer: The tracking says 'Delivered', but it's not here!
"""
print("\nDrafted Response:", draft_agent_response(conversation))

# Example 3: Summarize a ticket
ticket = """
Customer: My Echo Dot isn't connecting to Wi-Fi. I've tried restarting it and my router.
Order ID: #67890, purchased 2 weeks ago.
I need this fixed ASAP for an event tonight!
"""
print("\nTicket Summary:", summarize_ticket(ticket))

# Re-installing potentially conflicting libraries
!pip uninstall -y langchain langchain-community huggingface_hub transformers

# Re-installing with specific versions might be needed,
# but let's try the latest compatible ones first.
!pip install -q langchain-community huggingface_hub transformers faiss-cpu sentence-transformers pypdf openai tiktoken

# Set Hugging Face API Key (Replace with your token)
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

# Re-run the LLM initialization and the subsequent code blocks.
# If the error persists, investigate specific version compatibility
# of langchain-community and huggingface_hub.

# Option 1: Hugging Face Zephyr (Free)
# Remove the separate InferenceClient initialization.
# The HuggingFaceHub class from langchain-community handles the client internally.
llm = HuggingFaceHub(
    repo_id="HuggingFaceH4/zephyr-7b-beta",
    task="text-generation", # Specify the task
    model_kwargs={"temperature": 0.3, "max_length": 512}
)

# Option 2: OpenAI (Paid, better accuracy)
# os.environ["OPENAI_API_KEY"] = "your_openai_key"
# from langchain.llms import OpenAI
# llm = OpenAI(model="gpt-3.5-turbo-instruct")

# You can remove the separate InferenceClient example code that was causing confusion.
# If you need to use the direct huggingface_hub.InferenceClient for other tasks,
# keep that code separate from where you initialize the Langchain LLM.
# from huggingface_hub import InferenceClient
# client = InferenceClient(
#     provider="hf-inference",
#     api_key="hf_xxxxxxxxxxxxxxxxxxxxxxxx",
# )
#
# completion = client.chat.completions.create(
#     model="HuggingFaceH4/zephyr-7b-beta",
#     messages=[
#         {
#             "role": "user",
#             "content": "What is the capital of France?"
#         }
#     ],
# )
# print(completion.choices[0].message)

# Example 1: Retrieve KB answer
query = "How do I reset my Amazon password?"
answer = get_kb_answer(query)
print("Answer:", answer["answer"])
print("Source Pages:", answer["sources"])

# Example 2: Draft a response
conversation = """
Customer: I never received my order #12345. It was supposed to arrive yesterday.
Agent: Let me check the tracking details.
Customer: The tracking says 'Delivered', but it's not here!
"""
print("\nDrafted Response:", draft_agent_response(conversation))

# Example 3: Summarize a ticket
ticket = """
Customer: My Echo Dot isn't connecting to Wi-Fi. I've tried restarting it and my router.
Order ID: #67890, purchased 2 weeks ago.
I need this fixed ASAP for an event tonight!
"""
print("\nTicket Summary:", summarize_ticket(ticket))

!pip install -q gradio
import gradio as gr

# Mock functions if your real ones aren't working yet
def get_kb_answer(query):
    return {"answer": f"KB answer for: {query}", "sources": ["page1"]}

def draft_agent_response(conversation):
    return f"Drafted response for:\n{conversation}"

def summarize_ticket(ticket):
    return f"Summary of ticket:\n{ticket}"

def agent_assist_ui(query, conversation, ticket):
    try:
        answer = get_kb_answer(query)["answer"]
        draft = draft_agent_response(conversation)
        summary = summarize_ticket(ticket)
        return answer, draft, summary
    except Exception as e:
        return f"Error: {str(e)}", "", ""

iface = gr.Interface(
    fn=agent_assist_ui,
    inputs=[
        gr.Textbox(label="Search KB"),
        gr.Textbox(label="Conversation Context"),
        gr.Textbox(label="Ticket Text")
    ],
    outputs=[
        gr.Textbox(label="KB Answer"),
        gr.Textbox(label="Drafted Response"),
        gr.Textbox(label="Ticket Summary")
    ],
    title="Amazon Agent Assist Chatbot",
    examples=[
        ["How to reset password?", "Customer: I can't login...", "Order #123 missing"],
        ["Return policy", "Customer: I want to return...", "Defective item received"]
    ]
)

iface.launch(debug=True)  # debug=True shows more error details

!pip install -q gradio
import gradio as gr

# 1. Define your processing functions properly
def get_kb_answer(query):
    """Simulate KB lookup - replace with your actual function"""
    answers = {
        "How to reset password?": "Go to login page > Click 'Forgot Password' > Follow email instructions",
        "Return policy": "30-day return window for most items",
    }
    return {
        "answer": answers.get(query, "No answer found in KB"),
        "sources": ["KB Page 2"]
    }

def draft_agent_response(conversation):
    """Simulate response drafting - replace with your actual function"""
    return f"Suggested response to:\n{conversation}\n\n1. Acknowledge concern\n2. Provide solution\n3. Offer further help"

def summarize_ticket(ticket):
    """Simulate ticket summarization - replace with your actual function"""
    return f"Ticket Summary:\n- Issue: {ticket[:50]}...\n- Urgency: Medium\n- Category: Account"

# 2. Main processing function
def agent_assist_ui(query, conversation, ticket):
    try:
        # Process each input properly
        kb_result = get_kb_answer(query)
        draft_result = draft_agent_response(conversation)
        summary_result = summarize_ticket(ticket)

        return kb_result["answer"], draft_result, summary_result
    except Exception as e:
        return f"Error processing KB query: {str(e)}", "Error drafting response", "Error summarizing ticket"

# 3. Create interface with examples
iface = gr.Interface(
    fn=agent_assist_ui,
    inputs=[
        gr.Textbox(label="Search KB", placeholder="How to reset password?"),
        gr.Textbox(label="Conversation Context", placeholder="Customer: I can't login..."),
        gr.Textbox(label="Ticket Text", placeholder="Order #123 not received")
    ],
    outputs=[
        gr.Textbox(label="KB Answer"),
        gr.Textbox(label="Drafted Response"),
        gr.Textbox(label="Ticket Summary")
    ],
    title="Amazon Agent Assist Chatbot",
    examples=[
        ["How to reset password?", "Customer: I forgot my password", "Account login issues"],
        ["Return policy", "Customer: I want to return a defective item", "Order #456 defective product"]
    ]
)

# 4. Launch with debugging
iface.launch(debug=True)

"""#Integrated AI-Powered Agent Performance Monitoring System"""

# Re-installing potentially conflicting libraries with a focus on compatibility
!pip uninstall -y transformers sentence-transformers langchain langchain-community huggingface_hub

# Install known compatible versions or let pip resolve
# Using --upgrade ensures we get the latest compatible versions
!pip install --upgrade transformers sentence-transformers langchain-community huggingface_hub faiss-cpu pypdf openai tiktoken

# Set Hugging Face API Key
# Make sure to replace "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" with your actual key if it's different
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

# After running this cell, rerun the subsequent cells to re-initialize
# the LLM and other components, and then try the example calls again.
# If the issue persists, consider explicitly pinning versions of transformers
# and sentence-transformers that are known to work together.

# Core Imports (Building on previous chatbot code)
import pandas as pd
import numpy as np
from datetime import datetime
import json

# AI/ML Imports (Extending previous implementation)
from transformers import pipeline
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Visualization (New additions)
import plotly.express as px

# Install Dash and Dash Bootstrap Components
!pip install -q dash dash-bootstrap-components

import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import dash_bootstrap_components as dbc

# Import and extend previous chatbot components
# Assuming agent_chatbot.py exists and contains the functions/class
# from agent_chatbot import AgentChatbot, get_kb_answer, draft_agent_response, summarize_ticket

# Note: If agent_chatbot.py is not a separate file but the functions
# were defined in previous cells, you don't need the import line above.
# Ensure the functions get_kb_answer, draft_agent_response, and summarize_ticket
# are defined before this cell is run.

class InteractionLogger:
    def __init__(self, chatbot_instance):
        self.chatbot = chatbot_instance
        self.conversations = {}
        self.sentiment_analyzer = pipeline("sentiment-analysis")
        self.performance_metrics = pd.DataFrame(columns=[
            'conversation_id', 'agent_id', 'start_time', 'end_time',
            'resolution_status', 'customer_sentiment', 'guideline_compliance'
        ])

    def log_interaction(self, conversation_id, messages):
        """Record complete conversation from chatbot"""
        self.conversations[conversation_id] = messages

        # Analyze conversation quality
        analysis = self._analyze_conversation(messages)

        # Calculate handling time
        start = datetime.fromisoformat(messages[0]['timestamp'])
        end = datetime.fromisoformat(messages[-1]['timestamp'])
        handling_time = (end - start).total_seconds()

        # Store metrics
        self.performance_metrics = pd.concat([
            self.performance_metrics,
            pd.DataFrame([{
                'conversation_id': conversation_id,
                'agent_id': messages[-1]['agent_id'],
                'start_time': start,
                'end_time': end,
                'handling_time': handling_time,
                'resolution_status': analysis['resolved'],
                'customer_sentiment': analysis['avg_sentiment'],
                'guideline_compliance': analysis['compliance_score']
            }])
        ], ignore_index=True)

        return analysis

    def _analyze_conversation(self, messages):
        """Analyze conversation quality"""
        customer_messages = [m['content'] for m in messages if m['role'] == 'customer']
        agent_messages = [m['content'] for m in messages if m['role'] == 'agent']

        # Sentiment analysis
        sentiments = self.sentiment_analyzer(customer_messages)
        avg_sentiment = np.mean([s['score'] if s['label'] == 'POSITIVE' else -s['score'] for s in sentiments])

        # Check for resolution keywords
        resolved = any(keyword in agent_messages[-1].lower()
                      for keyword in ['resolved', 'solved', 'completed', 'finished'])

        # Compliance with guidelines (using previous chatbot's knowledge)
        compliance = self._check_guideline_compliance(agent_messages)

        return {
            'resolved': resolved,
            'avg_sentiment': avg_sentiment,
            'compliance_score': compliance
        }

    def _check_guideline_compliance(self, agent_messages):
        """Use chatbot's KB to verify guideline compliance"""
        compliance_checks = []

        # Check greeting
        greeting_check = any(msg.lower().startswith(('hello', 'hi', 'greetings'))
                            for msg in agent_messages[:1])
        compliance_checks.append(greeting_check)

        # Check empathy phrases
        empathy_check = any(any(phrase in msg.lower()
                             for phrase in ['sorry', 'understand', 'apologize'])
                            for msg in agent_messages)
        compliance_checks.append(empathy_check)

        # Check solution provided
        solution_check = any(any(phrase in msg.lower()
                               for phrase in ['solution', 'resolve', 'fix', 'answer'])
                            for msg in agent_messages)
        compliance_checks.append(solution_check)

        return np.mean(compliance_checks)

class IntegratedAgentSystem:
    def __init__(self):
        # Initialize chatbot components from previous implementation
        self.chatbot = AgentChatbot()
        self.logger = InteractionLogger(self.chatbot)
        self.dashboard = AgentPerformanceDashboard(self.logger)

        # Sample knowledge base (from previous implementation)
        self._init_knowledge_base()

    def _init_knowledge_base(self):
        """Initialize with sample KB from previous implementation"""
        self.chatbot.add_knowledge(
            "Amazon Return Policy",
            "Customers can return most items within 30 days of delivery"
        )
        self.chatbot.add_knowledge(
            "Amazon Shipping Policy",
            "Standard shipping takes 3-5 business days"
        )

    def process_customer_interaction(self, customer_query, agent_id="agent_001"):
        """Full processing pipeline"""
        # Step 1: Get KB answer (from previous implementation)
        kb_response = get_kb_answer(customer_query)

        # Step 2: Draft agent response (from previous implementation)
        conversation = [
            {"role": "customer", "content": customer_query, "timestamp": datetime.now().isoformat()}
        ]
        drafted_response = draft_agent_response(conversation)

        # Step 3: Log interaction
        conversation.append({
            "role": "agent",
            "content": drafted_response,
            "agent_id": agent_id,
            "timestamp": datetime.now().isoformat()
        })
        self.logger.log_interaction(f"conv_{len(self.logger.conversations)+1}", conversation)

        # Step 4: Summarize for ticket (from previous implementation)
        ticket_summary = summarize_ticket({
            "query": customer_query,
            "response": drafted_response
        })

        return {
            "response": drafted_response,
            "ticket_summary": ticket_summary,
            "performance_metrics": self.logger.performance_metrics.iloc[-1].to_dict()
        }

    def run_dashboard(self):
        """Launch performance monitoring dashboard"""
        self.dashboard.run()

if __name__ == "__main__":
    # Initialize integrated system
    system = IntegratedAgentSystem()

    # Simulate customer interactions
    interactions = [
        "How do I return an item?",
        "My package hasn't arrived after 5 days",
        "The product I received is damaged"
    ]

    for query in interactions:
        result = system.process_customer_interaction(query)
        print(f"\nQuery: {query}")
        print(f"Response: {result['response']}")
        print(f"Ticket Summary: {result['ticket_summary']}")
        print(f"Metrics: {json.dumps(result['performance_metrics'], indent=2)}")

    # Launch dashboard
    system.run_dashboard()

